{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * Often a part of Exploratory Data Analysis\n",
    " * Also ubiquituous for reporting on statistical results\n",
    " * Assumptions need to be met\n",
    " * There are problems with statistical testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "https://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The American Statistical Association put out an article with the following points:\n",
    "\n",
    " * P-values can indicate how incompatible the data are with a specified statistical model.\n",
    " * P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n",
    " * Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.Proper inference requires full reporting and transparency.\n",
    " * A p-value or statistical significance does not measure the size of an effect or the importance of a result.\n",
    " * By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ok, but what is Hypothesis testing? and what is a p-value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hypothesis tests are usually specified in the following way:\n",
    "\n",
    " * A Null Hypothesis which represents the status quo\n",
    " * An Alternative hypothesis which is what you're testing for\n",
    "\n",
    "Then, we conduct a hypothesis test *under the assumption* that the null hypothesis is true (this can take many forms)\n",
    "\n",
    "If the test results are extremely unlikely, we view that as evidence against the null hypothesis\n",
    "If they are likely, then perhaps the null hypothesis cannot be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's see an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have a patient populations and we are measuring their average A1C values in two separate years. Let's say in the first case that the average A1C is 7.2 with a sample size of 90 and a standard deviation of .36 and in the second case we have an average A1C of 8.1 with a standard deviation of .4 when we test 150 patients.\n",
    "\n",
    "What if we want to ask if the average A1C is different in this new population than the old one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a1c_1 = np.random.normal(7.2, .36, 90)\n",
    "a1c_2 = np.random.normal(8.1, .4, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(a1c_1), np.std(a1c_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(a1c_2), np.std(a1c_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ H_0: \\mu_1 = \\mu_2 $$\n",
    "$$ H_1: \\mu_1 \\neq \\mu2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a *p-value* is the probability of observing data as \"extreme\" or more extreme than we observe, *if the null hypothesis was true*\n",
    "\n",
    "If the p-value is lower than the significance level $\\alpha$ (we usually say lower than 0.05), we could conclude that it is unlikely that if the null hypothesis were true, that we would observe this value.\n",
    "\n",
    "In this case, we may reject the null hypothesis.\n",
    "\n",
    "If the p-value is higher than $\\alpha$, it would be fairly likely that we could observe the data that we do even if the null hypothesis were true, so we do not reject the null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hypothesis Test Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In all hypothesis tests, there are assumptions that have to be met. In this case, we need to assume that the two samples are random and independent, and that the distributions are normally distributed. In many cases, when we have large enough samples, certain tests are *robust* to these assumptions, so we can safely use them even if things are not perfectly normally distributed. It is important to look up the assumptions for the test you are using and make sure that the assumptions are either satisfied or the test is robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose a test and compute the test statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One of the most popular hypothesis tests is the $t$ test. The $t$ test is a test for the equality of means. For this, the test statistic is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$T=\\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S^2_1}{N_1} + \\frac{S^2_2}{N_2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\bar{X}$ are the sample means and $N$ are the sample sizes. If we assume that the standard deviations can differ, then $s^2_1$ and $s^2_2$ are the variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The standard error of *this test statistic* is given by just the denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we compute this, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = (np.mean(a1c_1)-np.mean(a1c_2))/np.sqrt(np.var(a1c_1, ddof = 1)/90 + np.var(a1c_2, ddof = 1)/150) \n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The test-statistic has a *known* distribution that we can now use to caculate the probability of seeing a T-statistic as extreme or more extreme than we have seen. The distribution in this case has a $t$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is also a concept known as degrees of freedom for the *t*-test. For now, we can just say that the degrees of freedom for this test is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def _unequal_var_ttest_denom(v1, n1, v2, n2):\n",
    "    vn1 = v1 / n1\n",
    "    vn2 = v2 / n2\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        df = (vn1 + vn2)**2 / (vn1**2 / (n1 - 1) + vn2**2 / (n2 - 1))\n",
    "\n",
    "    # If df is undefined, variances are zero (assumes n1 > 0 & n2 > 0).\n",
    "    # Hence it doesn't matter what df is as long as it's not NaN.\n",
    "    df = np.where(np.isnan(df), 1, df)\n",
    "    denom = np.sqrt(vn1 + vn2)\n",
    "    return df, denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df, se = _unequal_var_ttest_denom(np.var(a1c_1), 90, np.var(a1c_2), 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-3, 3, 100000)\n",
    "b = stats.t.pdf(a, df)\n",
    "fig, axes = plt.subplots()\n",
    "axes.scatter(a, b)\n",
    "axes.axvline(lower, linestyle = \":\", color = \"red\")\n",
    "axes.axvline(upper, linestyle = \":\", color = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats.t.cdf(t, math.floor(df)) * 2# For two-sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats.ttest_ind(a1c_1, a1c_2, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import weightstats as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.ttest_ind(a1c_1, a1c_2, alternative = \"two-sided\", usevar=\"unequal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Procedure for Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Set a Null and Alternative Hypothesis\n",
    "2. Choose a test and make sure that your data meets the assumptions\n",
    "2. Compute a test statistic \n",
    "3. Use the distribution of the test statistic to compute a p-value\n",
    "4. Check whether the p-value falls over or under the $\\alpha$ significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-sided vs. two-sided tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-sided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H_0: \\mu_1 = \\mu_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H_1: \\mu_1 < \\mu_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/onetailed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/twotailed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a p-value tells us nothing about the actual difference between the two means. This is one of the reasons why p-values are not enough to characterize the difference between two parameters. Because of this, oftentimes it is useful to generate a confidence interval.\n",
    "\n",
    "In this case, the confidence interval would be centered at the difference that we observed. If we want to generate a 95% confidence interval, we would select values of the T statistic's distribution:\n",
    "\n",
    "$$T \\sim t_{dof}$$\n",
    "\n",
    "such that the cdfs are between 0.025 and 0.975 (this encompasses 95%) and then add and subtract those from our sample mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = stats.t.ppf(0.025, math.floor(df))\n",
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = stats.t.ppf(0.975, math.floor(df))\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.linspace(-3, 3, 100000)\n",
    "b = stats.t.pdf(a, df)\n",
    "fig, axes = plt.subplots()\n",
    "axes.scatter(a, b)\n",
    "axes.axvline(lower, linestyle = \":\", color = \"red\")\n",
    "axes.axvline(upper, linestyle = \":\", color = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(a1c_1) - np.mean(a1c_2) + se*lower, np.mean(a1c_1) - np.mean(a1c_2) - se*lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sms.CompareMeans(sms.DescrStatsW(a1c_1), sms.DescrStatsW(a1c_2))\n",
    "print(cm.tconfint_diff(usevar='unequal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of errors in hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/errortypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/greenjellybean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsides to p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * p-values are dependent on sample size\n",
    " * p-values are NOT the probability that the null hypothesis is true!!!\n",
    " * Statistical Significance is not practical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = np.random.normal(1, 0.5, 100000)\n",
    "example_2 = np.random.normal(.98, 0.5, 100000)\n",
    "stats.ttest_ind(example_1, example_2, equal_var = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.hist(example_1, alpha = 0.2, color = \"blue\", bins= 50, label = 'example_1')\n",
    "axes.hist(example_2, alpha = 0.2, color = \"red\", bins = 50, label = 'example_2')\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misinterpretations of Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, people think that for a 95% confidence interval, there is a 95% probability that the true mean (or other parameter of interest) falls within the interval. However, the true mean is an unknown constant, *as are the endpoints of a confidence interval*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
